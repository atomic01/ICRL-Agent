{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:27:26.155443Z",
     "start_time": "2020-10-05T19:27:24.797732Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:27:26.785653Z",
     "start_time": "2020-10-05T19:27:26.156439Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(2))\n",
    "skip = tf.keras.layers.Dense(10,activation='relu')(inputs)\n",
    "for i in range(5):\n",
    "    layer = tf.keras.layers.Dense(10,activation='relu')(skip)\n",
    "    skip = tf.keras.layers.Add()([skip,layer])\n",
    "outputs = tf.keras.layers.Dense(4,activation='linear')(skip)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "def loss_func(model,inputs,value):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(inputs)\n",
    "        loss = tf.reduce_sum(tf.square(y_pred-value))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return loss, grads\n",
    "\n",
    "opt = tf.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:27:26.789509Z",
     "start_time": "2020-10-05T19:27:26.787105Z"
    }
   },
   "outputs": [],
   "source": [
    "#actions: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:27:53.811817Z",
     "start_time": "2020-10-05T19:27:53.781856Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  if rewards[current_row_index, current_column_index] == -1.:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    state = np.array([[current_row_index, current_column_index]])\n",
    "    y_pred = model(state / (N-1))\n",
    "    buffer = y_pred.numpy()\n",
    "    \n",
    "    done = False\n",
    "    while done != True:\n",
    "        new_state = state.copy()\n",
    "        if np.random.uniform() > epsilon:\n",
    "            action = np.argmax(buffer)\n",
    "        else:\n",
    "            action = np.floor(np.random.uniform(0,4)).astype(np.int32)\n",
    "        new_row_index, new_column_index = get_next_location(state[0][0],state[0][1], action)\n",
    "        if not is_terminal_state(new_row_index, new_column_index):\n",
    "            done = True  \n",
    "        else:\n",
    "            buffer[0][action] = -100\n",
    "            \n",
    "    return action, y_pred\n",
    "\n",
    "\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  return new_row_index, new_column_index\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else: \n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      action_index,_ = get_next_action(current_row_index, current_column_index, 1)\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "      if len(shortest_path) > (environment_rows * environment_columns):\n",
    "        return shortest_path\n",
    "    return shortest_path\n",
    "\n",
    "\n",
    "def draw_shortest_path():\n",
    "    shortest_path = get_shortest_path(0, 3)\n",
    "    if shortest_path:\n",
    "        for i in shortest_path:\n",
    "            data[i[0], i[1]] = 0.4\n",
    "\n",
    "    plt.imshow(data, interpolation='nearest', cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:27:54.121317Z",
     "start_time": "2020-10-05T19:27:53.996584Z"
    }
   },
   "outputs": [],
   "source": [
    "environment_rows = 10\n",
    "environment_columns = 10\n",
    "N = 10\n",
    "\n",
    "rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "\n",
    "maze = {} \n",
    "maze[0] = [3]\n",
    "maze[1] = [i for i in range(1,9)]\n",
    "maze[2] = [1, 6, 8]\n",
    "maze[3] = [1, 2, 3, 5, 6, 8]\n",
    "maze[4] = [1, 3, 5, 8]\n",
    "maze[5] = [1, 2, 3, 4, 5, 7, 8]\n",
    "maze[6] = [5]\n",
    "maze[7] = [1, 3, 5, 8]\n",
    "maze[8] = [i for i in range(1,9)]\n",
    "\n",
    "for row_index in range(0,N-1):\n",
    "    for column_index in maze[row_index]:\n",
    "        rewards[row_index, column_index] = -1\n",
    "rewards[9, 7] = 100\n",
    "        \n",
    "for row in rewards:\n",
    "    print(row)\n",
    "    \n",
    "data = np.zeros((environment_columns, environment_rows))\n",
    "for j in range(environment_rows):\n",
    "    for i in range(environment_columns):\n",
    "        if rewards[j][i] == -100:\n",
    "            data[j][i] = 0\n",
    "        else:\n",
    "            data[j][i] = 1\n",
    "\n",
    "plt.imshow(data, interpolation='nearest', cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:32:23.852842Z",
     "start_time": "2020-10-05T19:28:47.871894Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define training parameters\n",
    "N = environment_rows\n",
    "discount_factor = 0.95\n",
    "number_of_episodes = 1000\n",
    "STEPS = 500\n",
    "sampling_frequency = 10\n",
    "epsilon = 0.99\n",
    "\n",
    "#image pixel data\n",
    "original_data = data.copy()\n",
    "data_array = np.zeros((environment_rows, environment_columns, int(number_of_episodes/sampling_frequency) ))\n",
    "\n",
    "#metrics sotrage arrays\n",
    "reward_array = []\n",
    "episode_array_for_minmaxavg = []\n",
    "min_reward_array = [] \n",
    "max_reward_array = []\n",
    "avg_reward_array = []\n",
    "number_of_steps_array = []\n",
    "loss_array = []\n",
    "V=[]\n",
    "for episode in range(number_of_episodes):\n",
    "    row_index = 0\n",
    "    column_index = 3 \n",
    "    \n",
    "    number_of_steps = 0\n",
    "    aggr_reward = 0    \n",
    "    while not is_terminal_state(row_index, column_index) and number_of_steps < STEPS:\n",
    "        action, y_pred = get_next_action(row_index, column_index, epsilon)\n",
    "        old_row_index, old_column_index, old_action, old_y_pred = row_index, column_index, action, y_pred\n",
    "        old_state = np.array([[old_row_index, old_column_index]])\n",
    "        row_index, column_index = get_next_location(old_row_index, old_column_index, action)\n",
    "        \n",
    "        state = np.array([[row_index, column_index]])\n",
    "        reward = rewards[row_index, column_index]\n",
    "        \n",
    "        y_pred = model(state/(N-1))\n",
    "        max_pred = np.max(y_pred,axis=1)\n",
    "        value = reward + discount_factor * max_pred\n",
    "        V.append(value.squeeze())\n",
    "        value = value - np.mean(V)\n",
    "        value = value / (np.std(V) + 0.0000001)\n",
    "        \n",
    "        y_target = old_y_pred.numpy().copy()\n",
    "        y_target[0][old_action] = value\n",
    "        \n",
    "        loss, grads = loss_func(model,old_state/(N-1),y_target)\n",
    "        opt.apply_gradients(zip(grads,model.trainable_variables))\n",
    "                                        \n",
    "        #metrics variables-------------------------------------------------------\n",
    "        aggr_reward +=reward  \n",
    "        number_of_steps += 1\n",
    "        #------------------------------------------------------------------------\n",
    "        \n",
    "    if epsilon > 0.05:\n",
    "        epsilon *= 0.999\n",
    "        \n",
    "    \n",
    "    reward_array.append(aggr_reward)    \n",
    "    number_of_steps_array.append(number_of_steps)\n",
    "    loss_array.append(loss)\n",
    "    \n",
    "    if episode%sampling_frequency ==  0:\n",
    "        index = int(episode / sampling_frequency)  \n",
    "        draw_shortest_path()\n",
    "        data_array[:,:,index] = data.copy()\n",
    "        data = original_data.copy()\n",
    "        \n",
    "        average_reward = sum(reward_array[-sampling_frequency:])/sampling_frequency\n",
    "        min_reward = min(reward_array[-sampling_frequency:])\n",
    "        max_reward = max(reward_array[-sampling_frequency:])       \n",
    "        print(\"Episode: \", episode,\"Rewards-- min: \", min_reward ,\" max: \", max_reward, \" avg: \", average_reward, \" loss: \", loss)\n",
    "\n",
    "        episode_array_for_minmaxavg.append(episode)\n",
    "        min_reward_array.append(min_reward)\n",
    "        max_reward_array.append(max_reward)\n",
    "        avg_reward_array.append(average_reward)\n",
    "    \n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:37:57.170944Z",
     "start_time": "2020-10-01T21:37:50.238681Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 30))\n",
    "columns = 10\n",
    "rows = int(number_of_episodes / columns / sampling_frequency)\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = data_array[:,:,i-1]\n",
    "    fig.add_subplot(rows, columns, i)    \n",
    "    plt.imshow(img, interpolation='nearest', cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:33:22.589503Z",
     "start_time": "2020-10-01T20:33:22.422823Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 10))\n",
    "plt.plot(reward_array,  label=\"Reward\" )\n",
    "plt.ylim(-200, 100)\n",
    "plt.title(\"Rewards per episode\")\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:33:22.747640Z",
     "start_time": "2020-10-01T20:33:22.590855Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 10))\n",
    "plt.plot(episode_array_for_minmaxavg, min_reward_array, label=\"Min\" )\n",
    "plt.plot(episode_array_for_minmaxavg, max_reward_array, label=\"Max\" )\n",
    "plt.plot(episode_array_for_minmaxavg, avg_reward_array, label=\"Avg\" )\n",
    "plt.title(\"Reward MIN MAX AVG per episode\")\n",
    "plt.ylim(-200, 100)\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:33:22.918848Z",
     "start_time": "2020-10-01T20:33:22.748617Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 10))\n",
    "plt.plot(number_of_steps_array,  label=\"Number of steps\" )\n",
    "plt.title(\"Number of steps per episode\")\n",
    "plt.ylim(-50, 100)\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T19:11:59.134020Z",
     "start_time": "2020-10-05T19:11:58.997980Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 10))\n",
    "plt.plot(loss_array,  label=\"Loss\" )\n",
    "plt.title(\"Loss per episode\")\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
